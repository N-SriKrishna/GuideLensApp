# ğŸ“± GuideLensApp: AI-Powered Visual Navigation for Android

**GuideLensApp** is an accessibility-focused on-device navigation system that helps visually impaired users navigate indoor environments independently.  
Built entirely in **Kotlin** with **Jetpack Compose**, it combines real-time **object detection**, **semantic floor segmentation**, and **intelligent pathfinding** to deliver turn-by-turn **audio guidance** through an intuitive AR interface â€” all processed locally on the device.

---

## ğŸŒŸ Why GuideLensApp?

âœ… **100 % On-Device** â€” No internet required, full privacy  
âœ… **Audio-First Design** â€” Text-to-Speech announcements for all navigation events  
âœ… **Production-Ready** â€” Device-adaptive configuration, robust error handling  
âœ… **Optimized Performance** â€” INT8 quantization, NNAPI acceleration, 15â€“20 FPS  
âœ… **Open Source** â€” GPL-3.0 license, fully documented and community-driven  

---

## ğŸš€ Core Features

### ğŸ§  Computer Vision & Navigation

- **Real-Time Object Detection** â€“ YOLO World v2 (INT8) detects 80 object classes at 640Ã—640 resolution with 150â€“250 ms latency.  
  Supports 16 navigable targets: `chair, door, table, bed, couch, toilet, sink, refrigerator, stairs, person, bottle, cup, laptop, phone, keyboard, mouse`.

- **Semantic Floor Segmentation** â€“ Custom-trained **PP-LiteSeg (INT8)** identifies walkable surfaces at 256Ã—256 resolution, achieving 4Ã— smaller size and 2â€“3Ã— faster inference than FP32.

- **Intelligent Pathfinding** â€“ **A\*** search on a down-sampled 15 px grid with 1-cell obstacle inflation.  
  Computes optimal paths in 20â€“40 ms with guaranteed termination (â‰¤ 10 000 iterations).

- **Pure Pursuit Control** â€“ Robotics-grade trajectory tracking with 100 px look-ahead; generates natural commands:  
  *â€œMove straight forwardâ€*, *â€œTurn slightly leftâ€*, *â€œTurn sharply rightâ€*.

---

## â™¿ Accessibility Features

- **Text-to-Speech Integration**
  - â€œNavigating to [object]â€ on start  
  - â€œ[Object] foundâ€ on first detection  
  - Turn commands every 3 s (max)  
  - â€œArrived at destinationâ€ on goal  
  - â€œNavigation stoppedâ€ on exit

- **Scene Description** â€“ On-demand environment summary:  
  *â€œI can see 2 chairs, 1 table, and 1 door.â€*

- **Voice Command Framework (Ready)** â€“ Architecture supports:  
  *â€œNavigate to [object]â€*, *â€œWhat do you see?â€*, *â€œRead textâ€*, *â€œStop navigationâ€*

- **Augmented Reality Overlays** â€“ For sighted guides:  
  Green floor masks (60 % alpha), bounding boxes, cyan path lines, red target crosshairs.

---

## ğŸ› ï¸ Technology Stack

| Component | Technology | Details |
|------------|-------------|----------|
| **Platform** | Android API 24+ | Nougat 7.0 and later |
| **Language** | Kotlin 100 % | Modern coroutines-based |
| **UI** | Jetpack Compose | Material Design 3 UI |
| **Architecture** | MVVM | `ViewModel`, `StateFlow` separation |
| **ML Runtime** | ONNX Runtime 1.16.0 | Cross-platform INT8 optimized |
| **Object Detection** | YOLO World v2 (INT8) | 80 classes (~10 MB), NNAPI accel |
| **Segmentation** | PP-LiteSeg (INT8) | Custom trained (~3 MB) CPU opt |
| **Camera** | CameraX 1.3+ | `STRATEGY_KEEP_ONLY_LATEST` |
| **Concurrency** | Kotlin Coroutines | Custom dispatchers + ThreadPoolExecutor |

**Model Pipeline**

- **YOLO World v2** â†’ PyTorch â†’ ONNX â†’ INT8 Quantization  
- **PP-LiteSeg** â†’ Custom PyTorch Training â†’ ONNX â†’ INT8 Quantization  

**Why ONNX Runtime?**  
15â€“20 % faster INT8 inference than TFLite, superior NNAPI integration, cross-platform portability.

---

## ğŸ§­ Algorithms & Implementation

### ğŸ—ºï¸ A\* Pathfinding
- Manhattan-distance heuristic, 8-directional movement (straight = 10, diagonal = 14)  
- 1-cell obstacle inflation for safety margins  
- BFS fallback for invalid start/goal positions  
- Executes in 20â€“40 ms on 85Ã—48 grid  

### ğŸ”„ Pure Pursuit Controller
- Curvature Îº = 2 Ã— sin Î± / L (100 px look-ahead)  
- Thresholds: sharp > 0.05, slight > 0.02  
- Generates discrete commands for TTS clarity  
- Stable convergence without oscillation  

---

## âš™ï¸ Device-Adaptive Configuration

| Tier | FPS | Resolution | ML Threads | Acceleration |
|------|-----|-------------|-------------|---------------|
| **High-End** (â‰¥ 8 GB RAM, â‰¥ 8 cores) | 20 | 1280Ã—720 | 4 | NNAPI + FP16 |
| **Mid-Range** (4â€“6 GB RAM) | 15 | 960Ã—540 | 2 | CPU only INT8 |

Dynamic profiling adjusts thresholds, frame rates, and resolution at runtime.

---

## ğŸ“¦ Installation

### Prerequisites
- Android Studio Hedgehog (2023.1.1+)  
- Android SDK API 24+  
- Physical device with camera (â‰¥ 4 GB RAM recommended 8 GB)

### Setup
bash
git clone https://github.com/N-SriKrishna/GuideLensApp.git
cd GuideLensApp

## ğŸ“¦ Installation

### Add ML models to `app/src/main/assets/`
 - yolov8s-worldv2_int8.onnx (~10 MB)
 - floor_segmentation_int8.onnx (~3 MB)

### Build & Run
1. **File â†’ Sync Project with Gradle**  
2. **Build â†’ Make Project**  
3. **Run â†’ Run 'app'** (grant camera permission)

---

## â–¶ï¸ Usage

### Basic Navigation
1. Tap âš™ï¸ to select target object (e.g. *Chair*)  
2. Tap â€œStart Navigationâ€ â†’ Hear â€œNavigating to chairâ€  
3. Follow audio commands: â€œTurn slightly leftâ€, â€œMove forwardâ€  
4. Arrival â†’ â€œArrived at destinationâ€  
5. Stop â†’ â€œNavigation stoppedâ€  

**Scene Description** â€“ Tap ğŸ“„ to hear object summary  
**Visual Overlays** â€“ Green floors, cyan paths, bounding boxes, red crosshair  

---

## âš™ï¸ Performance & Optimization

### Benchmarks (Samsung Galaxy S23, Snapdragon 8 Gen 2)
| Component | Latency | Notes |
|------------|----------|-------|
| Object Detection | 150â€“250 ms | YOLO World INT8 + NNAPI |
| Floor Segmentation | 80â€“120 ms | PP-LiteSeg INT8 |
| Path Planning | 20â€“40 ms | A* on 85Ã—48 grid |
| End-to-End | 250â€“400 ms | 2.5â€“4 FPS pipeline |

Memory â‰ˆ **250 MB (active)** Â· Battery â‰ˆ **15â€“20 % per hour**

### Key Optimizations
- **INT8 Quantization** â†’ 4Ã— smaller models, 2â€“3Ã— faster inference  
- **NNAPI Acceleration** â†’ 2Ã— speedup on Snapdragon NPUs  
- **Frame Throttling** â†’ Adaptive 50â€“67 ms intervals  
- **Memory Pooling** â†’ Bitmap reuse + explicit GC  
- **Grid Down-sampling** â†’ 100Ã— fewer cells for A*  

---

## ğŸ—ï¸ Project Architecture
app/src/main/java/com/example/guidelensapp/
â”œâ”€â”€ MainActivity.kt # Entry, permissions, Compose setup
â”œâ”€â”€ GuideLensApplication.kt # App init, crash handler
â”œâ”€â”€ Config.kt # Device-adaptive settings
â”œâ”€â”€ viewmodel/
â”‚ â”œâ”€â”€ NavigationViewModel.kt # Pipeline control, TTS integration
â”‚ â””â”€â”€ NavigationUiState.kt # UI state (StateFlow)
â”œâ”€â”€ ml/
â”‚ â”œâ”€â”€ ObjectDetector.kt # YOLO World ONNX inference
â”‚ â””â”€â”€ ONNXFloorSegmenter.kt # PP-LiteSeg ONNX inference
â”œâ”€â”€ sensors/
â”‚ â”œâ”€â”€ SpatialTracker.kt 
â”œâ”€â”€ navigation/
â”‚ â””â”€â”€ PathPlanner.kt # A* + Pure Pursuit
â”œâ”€â”€ accessibility/
â”‚ â””â”€â”€ TextToSpeechManager.kt # TTS wrapper
â”œâ”€â”€ utils/
â”‚ â”œâ”€â”€ ThreadManager.kt # Thread pools (ML thread max priority)
â”‚ â””â”€â”€ MemoryManager.kt # Bitmap pool + GC control
â””â”€â”€ ui/composables/
â”œâ”€â”€ CameraView.kt # CameraX integration
â”œâ”€â”€ OverlayCanvas.kt # AR visualization
â””â”€â”€ ObjectSelectorView.kt # Target selection dialog

**Data Flow:**  
Camera Frame â†’ ViewModel â†’ [SensorTracker + ObjectDetector â†’ FloorSegmenter â†’ PathPlanner] â†’ TTS â†’ StateFlow â†’ Compose UI

## ğŸ§© Engineering Highlights

- **Asynchronous Initialization** â€“ Coroutines + custom dispatchers prevent UI blocking  
- **Robust Error Handling** â€“ Fallback modes if models fail to load  
- **Thread Optimization** â€“ Single ML thread avoids context switch overhead  
- **Memory Safety** â€“ WeakReference bitmap pool + forced GC on crash  
- **High Code Quality** â€“ Null-safe Kotlin, clean MVVM separation, structured logging  

---

## ğŸ¯ Future Enhancements

- **Distance Estimation** (â€œObject 2 m aheadâ€)  
- **Dynamic Obstacle Avoidance** with real-time re-planning  
- **OCR Text Reading** (ML Kit integration)  
- **Haptic Turn Cues** for tactile feedback  
- **Full Voice Command Support** for hands-free interaction  
- **Multi-Object Waypoints & Navigation History**  
- **Sensor Fusion** for improved heading stability  
- **Model Distillation** for total model size under 5 MB  

---

## ğŸ’¡ Accessibility & Impact

**GuideLensApp** empowers users with visual impairments to navigate independently using only a smartphone camera â€” no beacons, maps, or internet required.  
It demonstrates that **real-time, privacy-preserving AI** can be practical on-device, enhancing inclusion and mobility for millions worldwide.

---

## ğŸ“š References & Resources

- [ONNX Runtime Mobile Docs](https://onnxruntime.ai/docs/get-started/with-mobile.html)  
- [Ultralytics YOLO World Docs](https://docs.ultralytics.com/hub/app/android/)  
- [PP-LiteSeg Paper (ArXiv)](https://arxiv.org/html/2504.20976v1)  
- [A* Algorithm â€“ Wikipedia](https://en.wikipedia.org/wiki/A*_search_algorithm)  
- [Pure Pursuit Controller â€“ MathWorks](https://www.mathworks.com/help/robotics/ref/purepursuit.html)  

---

## ğŸ§‘â€ğŸ’» Author & Repository

**Developer:** Sri Krishna Nurandu  
**Repository:** [github.com/N-SriKrishna/GuideLensApp](https://github.com/N-SriKrishna/GuideLensApp)  
**License:** [GPL-3.0](https://www.gnu.org/licenses/gpl-3.0.html)  

---

## ğŸ Conclusion

**GuideLensApp** showcases the future of **on-device AI navigation** â€” merging deep learning, classical algorithms, and accessibility design into one cohesive Android application.  
With optimized models, adaptive runtime, and robust engineering, it stands as a **reference implementation for real-time computer-vision navigation** on mobile devices.

